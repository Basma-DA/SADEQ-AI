# -*- coding: utf-8 -*-
"""Graduation_project_final_tryeee.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sJ3K11SG0ZPdyl3gqUFsb5xrW5kpSN4c
"""

!pip install transformers sentencepiece
!pip install num2words
!pip install deep-translator
!pip install torch openpyxl
!pip install ydata-profiling
!pip install googletrans==4.0.0-rc1 tqdm
!pip install emoji --quiet

"""## Import libraries"""

import re
import os
import json
import pandas as pd
from ydata_profiling import ProfileReport
from nltk.corpus import stopwords
from nltk.stem.arlstem import ARLSTem
from bs4 import BeautifulSoup
from num2words import num2words
from deep_translator import GoogleTranslator
from tqdm.notebook import tqdm
from multiprocessing import Pool, cpu_count
import torch
import emoji
from num2words import num2words
from googletrans import Translator
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from multiprocessing import Pool
import numpy as np

"""## Loading Dataset"""

#Load dataset
df = pd.read_excel('/content/correct_balanced_data_ffinnall.xlsx')
df.dropna(subset=['news', 'is_fake'], inplace=True)
df.head()

df.columns

# Generate the profile report
profile = ProfileReport(df, title="Pandas Profiling Report", explorative=True)

profile.to_notebook_iframe()

# Save the report to HTML
profile.to_file("my_report.html")

df['is_fake'].value_counts()

df.shape

"""## Data Exploration Visualizations"""

import matplotlib.pyplot as plt
import seaborn as sns

# Class distribution
sns.countplot(x='is_fake', data=df)
plt.title("Fake vs Real News Distribution")
plt.xlabel("Label (0 = Real, 1 = Fake)")
plt.ylabel("Count")
plt.show()

# Text length distribution
df['text_length'] = df['news'].apply(lambda x: len(str(x)))
plt.figure(figsize=(8,5))
sns.histplot(df['text_length'], bins=50, kde=True)
plt.title("Distribution of News Text Lengths")
plt.xlabel("Number of Characters")
plt.ylabel("Frequency")
plt.show()

# Word count distribution
df['word_count'] = df['news'].apply(lambda x: len(str(x).split()))
plt.figure(figsize=(8,5))
sns.histplot(df['word_count'], bins=50, kde=True)
plt.title("Distribution of Word Counts")
plt.xlabel("Number of Words")
plt.ylabel("Frequency")
plt.show()

"""## Preprocessing

**preprocessing pipline**:

Cleaning + Normalization

Summarization

    
Translation (for English words)

   
Cleaned Text (human readable)

Tokenization â†’  Used as input to model
"""

# Imports

import pandas as pd
import re
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer, TrainingArguments
from tqdm import tqdm
from googletrans import Translator
from bs4 import BeautifulSoup
import torch
from num2words import num2words

# Setup

from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

# ØªØ­Ù…ÙŠÙ„ Tokenizer Ø§Ù„Ø®Ø§Øµ Ø¨Ù€ BERT Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ù„ØºØ§Øª
tokenizer = AutoTokenizer.from_pretrained("bert-base-multilingual-cased")

# ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ BERT Ù„Ù„ØªØ­Ù„ÙŠÙ„ Ø£Ùˆ Ø§Ù„ØªØµÙ†ÙŠÙ (Ø¹Ø¯Ø¯ Ø§Ù„ÙØ¦Ø§Øª: 2 - ØµØ­ÙŠØ­Ø© / ÙƒØ§Ø°Ø¨Ø©)
model = AutoModelForSequenceClassification.from_pretrained("bert-base-multilingual-cased", num_labels=2)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

def convert_numbers_to_words(text):
    def replace_number(match):
        try:
            num = int(match.group())
            return num2words(num, lang='ar')
        except:
            return match.group()
    return re.sub(r'\d+', replace_number, text)

# Clean and normalize Arabic text
def clean_text(text):
    if not isinstance(text, str):
        return ""
    text = re.sub(r"http\S+|www.\S+", "", text)  # Remove URLs
    text = BeautifulSoup(text, "html.parser").get_text()  # Remove HTML tags
    text = convert_numbers_to_words(text)  # Convert digits to words
    text = re.sub(r'[^\w\s\u0600-\u06FF]', ' ', text)  # Remove non-Arabic characters
    text = re.sub(r'[Ø¥Ø£Ø¢Ø§]', 'Ø§', text)
    text = re.sub(r'Ù‰', 'ÙŠ', text)
    text = re.sub(r'Ø¤', 'Ùˆ', text)
    text = re.sub(r'Ø¦', 'ÙŠ', text)
    # Do not replace "Ø©" to preserve meaning

    text = re.sub(r'\s+', ' ', text).strip()
    return text

def remove_emoji(text):
    return emoji.replace_emoji(text, replace='')

def translate_english_to_arabic(text):
    translator = Translator()

    words = text.split()
    english_words = list({word for word in words if re.search(r'[a-zA-Z]', word)})
    if not english_words:
        return text
    try:
        translations = translator.translate(english_words, src='en', dest='ar')
        translated_dict = {eng: trans.text for eng, trans in zip(english_words, translations)}
    except:
        translated_dict = {word: word for word in english_words}  # fallback ÙÙŠ Ø­Ø§Ù„ ÙØ´Ù„ Ø§Ù„ØªØ±Ø¬Ù…Ø©
    translated_words = [translated_dict.get(word, word) for word in words]
    translated_text = ' '.join(translated_words).replace('\n', ' ')
    return translated_text.strip()

# Full preprocessing pipeline
def preprocess_pipeline(text):
    # ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù†ØµÙˆØµ ØºÙŠØ± Ø§Ù„ØµØ§Ù„Ø­Ø©
    def is_valid(text):
        text = text.strip()
        return text and len(text) > 10 and not re.fullmatch(r'[\[\]{}()\-_.\s]*', text.lower()) and "content" not in text.lower()

    if not is_valid(text):
        return ""
    cleaned = clean_text(text)
    removed = remove_emoji(cleaned)
    translated = translate_english_to_arabic(removed)  # ØªØ±Ø¬Ù…Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©
    return translated

# Main function: read, process, and export

def main():
    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    df = pd.read_excel("/content/correct_balanced_data_ffinnall.xlsx")
    df.dropna(subset=['news'], inplace=True)


    print("Sample before preprocessing:")
    print(df["news"].head(3))

    # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø¨Ø±ÙŠØ¨Ø±ÙˆØ³ÙŠØ³ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø´Ø±ÙŠØ· Ø§Ù„ØªÙ‚Ø¯Ù…
    tqdm.pandas()
    print("\nRunning preprocessing pipeline with progress bar...")
    df["cleaned_news"] = df["news"].progress_apply(preprocess_pipeline)

    print("\nSample after preprocessing:")
    print(df[["news", "cleaned_news"]].head(3))

    # Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
    df.to_excel("cleaned_news_all_good_correct_withoutsum_final.xlsx", index=False)
    print("Saved as cleaned_news_all_good_correct_withoutsum_final.xlsx")

# ØªØ´ØºÙŠÙ„ Ø§Ù„Ø³ÙƒØ±Ø¨Øª
if __name__ == '__main__':
    main()

df_cleaned = pd.read_excel("/content/cleaned_news_all_good_correct_withoutsum_final.xlsx")

# Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª ÙÙŠ Ø§Ù„Ø¹Ù…ÙˆØ¯ is_fake
print("ğŸ” Sample of class labels from your dataset:")
print(df_cleaned['is_fake'].value_counts())
print("\nğŸ“ Sample rows to verify meaning of labels:")

# Ø·Ø¨Ø§Ø¹Ø© Ø¹ÙŠÙ†Ø§Øª Ù…Ù† ÙƒÙ„ ØªØµÙ†ÙŠÙ
for label in [0, 1]:
    label_name = "Real" if label == 0 else "Fake"
    sample_text = df_cleaned[df_cleaned['is_fake'] == label]['cleaned_news'].head(1).values[0]
    print(f"\nLabel: {label} â¤ Interpreted as: {label_name}\nSample text:\n{sample_text[:300]}...")

"""## Tokenization"""

!pip install --upgrade transformers

#Import libraries
import torch
from torch.utils.data import Dataset
from sklearn.model_selection import train_test_split
import pandas as pd
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
from torch.utils.data import Dataset

df_cleaned = pd.read_excel("/content/cleaned_news_all_good_correct_withoutsum_final.xlsx")
df_cleaned.head()

# Text length distribution

import matplotlib.pyplot as plt
import seaborn as sns
df_cleaned['text_length'] = df_cleaned['cleaned_news'].apply(lambda x: len(str(x)))
plt.figure(figsize=(8,5))
sns.histplot(df_cleaned['text_length'], bins=50, kde=True)
plt.title("Distribution of News Text Lengths")
plt.xlabel("Number of Characters")
plt.ylabel("Frequency")
plt.show()

# Word count distribution
df_cleaned['word_count'] = df_cleaned['cleaned_news'].apply(lambda x: len(str(x).split()))
plt.figure(figsize=(8,5))
sns.histplot(df_cleaned['word_count'], bins=50, kde=True)
plt.title("Distribution of Word Counts")
plt.xlabel("Number of Words")
plt.ylabel("Frequency")
plt.show()

df_cleaned.columns

df_cleaned=df_cleaned.drop('news', axis=1)
df_cleaned = df_cleaned.dropna(subset=['cleaned_news'])  # Drop any empty news rows

df_cleaned.head()

from transformers import AutoTokenizer, AutoModelForSequenceClassification

model_name = "aubmindlab/bert-base-arabertv2"

tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)

max_len=512

"""## Modeling

## Step 1: Split + Dataset Class
"""

from torch.utils.data import DataLoader, WeightedRandomSampler
from sklearn.model_selection import train_test_split
from transformers import AutoTokenizer

# Ø£ÙˆÙ„Ø§Ù‹: ØªØ£ÙƒØ¯ÙŠ Ø£Ù† Ø¹Ù…ÙˆØ¯ Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ø³Ù…Ù‡ ØµØ­ÙŠØ­ (Ù…Ø«Ù„Ø§Ù‹: 'is_fake' Ø£Ùˆ 'label' Ø­Ø³Ø¨ Ø­Ø§Ù„ØªÙƒ)
label_column = 'is_fake'  # ØºÙŠØ±ÙŠÙ‡ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ø³Ù… Ø§Ù„Ø¹Ù…ÙˆØ¯ Ù…Ø®ØªÙ„Ù

# Ø§Ù„Ø®Ø·ÙˆØ© 1: ØªÙ‚Ø³ÙŠÙ… train Ùˆ val+test Ù…Ø¹ stratify
train_df_cleaned, val_test_df_cleaned = train_test_split(
    df_cleaned,
    test_size=0.2,
    stratify=df_cleaned[label_column],
    random_state=42
)

# Ø§Ù„Ø®Ø·ÙˆØ© 2: ØªÙ‚Ø³ÙŠÙ… val Ùˆ test Ù…Ù† val_test Ù…Ø¹ stratify Ø£ÙŠØ¶Ø§Ù‹
val_df, test_df = train_test_split(
    val_test_df_cleaned,
    test_size=0.5,
    stratify=val_test_df_cleaned[label_column],
    random_state=42
)

print("Train distribution:")
print(train_df_cleaned[label_column].value_counts())
print("\nValidation distribution:")
print(val_df[label_column].value_counts())
print("\nTest distribution:")
print(test_df[label_column].value_counts())

import numpy as np
import torch
from torch.utils.data import DataLoader, WeightedRandomSampler, Dataset
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

class NewsDataset(Dataset):
    def __init__(self, dataframe, tokenizer, max_len):
        self.texts = dataframe['cleaned_news'].values
        self.labels = dataframe['is_fake'].values
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]

        # Tokenize the text using the tokenizer
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_len,
            return_tensors='pt'
        )

        # Extracting the encoded input and attention mask
        input_ids = encoding['input_ids'].squeeze(0)  # [seq_len]
        attention_mask = encoding['attention_mask'].squeeze(0)

        # Convert label to tensor
        labels_tensor = torch.tensor(label, dtype=torch.long)

        return {
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'labels': labels_tensor
        }

def create_balanced_dataloader(train_df, val_df, test_df, tokenizer, max_len, batch_size=16):
    """
    Creates dataloaders for training, validation, and testing using the NewsDataset.
    Assumes the data is already balanced.
    """
    # Create dataset objects
    train_dataset = NewsDataset(train_df, tokenizer, max_len)
    val_dataset = NewsDataset(val_df, tokenizer, max_len)
    test_dataset = NewsDataset(test_df, tokenizer, max_len)

    # Create dataloaders
    train_dataloader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,      # âœ… ÙÙ‚Ø· shuffle ÙƒØ§ÙÙŠ
        drop_last=True,
        num_workers=2,
        pin_memory=True
    )

    val_dataloader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        drop_last=False,
        num_workers=2,
        pin_memory=True
    )

    test_dataloader = DataLoader(
        test_dataset,
        batch_size=batch_size,
        shuffle=False,
        drop_last=False,
        num_workers=2,
        pin_memory=True
    )

    # Calculate number of batches
    import math
    num_train_batches = math.ceil(len(train_dataset) / batch_size)
    num_val_batches = math.ceil(len(val_dataset) / batch_size)
    num_test_batches = math.ceil(len(test_dataset) / batch_size)

    print("Number of train batches:", num_train_batches)
    print("Number of validation batches:", num_val_batches)
    print("Number of test batches:", num_test_batches)

    return train_dataloader, val_dataloader, test_dataloader, train_dataset, val_dataset, test_dataset

def verify_batch_balance(dataloader, num_batches=10, num_classes=2):
    """
    Checks the class balance across batches and visualizes the results.

    Args:
        dataloader: The dataloader to check
        num_batches: Number of batches to check
        num_classes: Number of classes in the dataset
    """
    batch_class_counts = []
    label_counts = {i: 0 for i in range(num_classes)}

    print(f"\nChecking class balance in {num_batches} batches:")

    for i, batch in enumerate(dataloader):
        if i >= num_batches:
            break

        # Get batch labels
        batch_labels = batch['labels'].cpu().numpy()
        unique_labels, counts = np.unique(batch_labels, return_counts=True)

        # Store batch distribution
        batch_dist = {label: 0 for label in range(num_classes)}  # Initialize with all classes
        for label, count in zip(unique_labels, counts):
            batch_dist[label] = count
            label_counts[label] += count

        batch_class_counts.append(list(batch_dist.values()))

        # Print batch information
        print(f"Batch {i+1}: " + ", ".join([f"Class {label}: {batch_dist[label]} samples" for label in range(num_classes)]))

    # Calculate overall statistics
    total_samples = sum(label_counts.values())
    for label in range(num_classes):

        percent = (label_counts[label] / total_samples) * 100 if total_samples > 0 else 0
        print(f"Class {label}: {label_counts[label]} samples ({percent:.1f}%)")

    # Calculate balance ratio
    min_count = min(label_counts.values())
    max_count = max(label_counts.values())
    balance_ratio = min_count / max_count if max_count > 0 else 0
    print(f"Balance ratio: {balance_ratio:.3f} (1.0 is perfect balance)")

    # Visualization (same as before)
    batch_class_counts = np.array(batch_class_counts)

    plt.figure(figsize=(12, 7))

    ax1 = plt.subplot(2, 1, 1)
    x = np.arange(len(batch_class_counts))
    width = 0.35
    for label in range(num_classes):
        ax1.bar(x + width*label - width*num_classes/2, batch_class_counts[:, label], width, label=f'Class {label}')
    ax1.set_xlabel('Batch Number')
    ax1.set_ylabel('Number of Samples')
    ax1.set_title('Class Distribution Across Batches')
    ax1.set_xticks(x)
    ax1.set_xticklabels([f'Batch {i+1}' for i in range(len(batch_class_counts))])
    ax1.legend()

    ax2 = plt.subplot(2, 1, 2)
    ax2.pie([label_counts[label] for label in range(num_classes)],
            labels=[f'Class {label}' for label in range(num_classes)],
            autopct='%1.1f%%',
            startangle=90,
            explode=[0.05]*num_classes)
    ax2.set_title('Overall Class Distribution')

    plt.tight_layout()
    plt.show()

    return balance_ratio

# Create balanced dataloaders and get dataset objects
train_dataloader, val_dataloader, test_dataloader, train_dataset, val_dataset, test_dataset = create_balanced_dataloader(
    train_df_cleaned,
    val_df,
    test_df,
    tokenizer=tokenizer,  # Your BERT tokenizer
    max_len=max_len,      # Your maximum sequence length
    batch_size=16
)

#  Verify batch balance
verify_batch_balance(train_dataloader, num_batches=16)

"""## Step 2: DataLoaders, Model, Training Loop"""

import torch
from torch import nn, optim
from torch.utils.data import DataLoader
from torch.optim import AdamW # Import AdamW from torch.optim instead of transformers
from transformers import get_linear_schedule_with_warmup # Keep this import for the scheduler
from tqdm import tqdm

import torch
from transformers import AutoConfig, AutoModelForSequenceClassification
import os

# Enable expandable memory allocation to reduce fragmentation
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

#  Model Setup (Load ArabBERT with custom dropout for better generalization)
config = AutoConfig.from_pretrained('aubmindlab/bert-base-arabertv2',
                                    num_labels=2,
                                    hidden_dropout_prob=0.2,
                                    attention_probs_dropout_prob=0.2)

model = AutoModelForSequenceClassification.from_pretrained('aubmindlab/bert-base-arabertv2', config=config)

# 2. Set device (GPU or CPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

from torch.optim import AdamW
from transformers import get_linear_schedule_with_warmup

# Set Optimizer with weight decay to reduce overfitting (L2 regularization is handled by AdamW)
optimizer = AdamW(
    model.parameters(),         # Model parameters
    lr=2e-5,                    # Learning rate
    eps=1e-8,                   # Epsilon (for numerical stability)
    weight_decay=0.01           # Weight decay for L2 regularization
)

# Set the number of epochs (30 epochs)
epochs = 30

# Calculate total number of training steps
total_steps = len(train_dataloader) * epochs  # ØªØºÙŠÙŠØ± Ù…Ù† 10 Ø¥Ù„Ù‰ 30 epochs

# Set the warmup steps (10% of total steps)
num_warmup_steps = int(0.1 * total_steps)  # 10% warmup steps

# Initialize the scheduler with warmup
scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=num_warmup_steps,
    num_training_steps=total_steps
)

# No need for get_regularization_loss anymore, as weight_decay handles L2 regularization in AdamW

# Define loss and accuracy functions
import torch

def compute_accuracy(preds, labels):
    # Get the index of the max probability for each prediction (argmax for classification)
    pred_flat = torch.argmax(preds, dim=1).flatten()
    labels_flat = labels.flatten()

    # Calculate correct predictions
    correct_predictions = (pred_flat == labels_flat).sum().item()

    # Return accuracy as a fraction
    accuracy = correct_predictions / len(labels_flat)
    return accuracy

"""## Training"""

from tqdm import tqdm
import torch
import matplotlib.pyplot as plt

def train(model, train_loader, val_loader, optimizer, scheduler, num_epochs, tokenizer, patience=2):
    train_losses = []
    val_losses = []
    train_accuracies = []
    val_accuracies = []
    best_val_loss = float("inf")
    patience_counter = 0
    best_model_state = None

    for epoch in range(num_epochs):
        print(f"\nEpoch {epoch + 1}/{num_epochs}")
        model.train()
        total_loss = 0
        correct_preds = 0
        total_preds = 0

        for batch in tqdm(train_loader, desc="Training"):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            # Forward pass
            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
            loss = outputs.loss
            logits = outputs.logits

            total_loss += loss.item()

            # Calculate training accuracy
            preds = torch.argmax(logits, dim=1)
            correct_preds += (preds == labels).sum().item()
            total_preds += labels.size(0)

            # Backward pass
            loss.backward()
            optimizer.step()
            scheduler.step()
            optimizer.zero_grad()
            torch.cuda.empty_cache()

        # Record average training loss and accuracy
        avg_train_loss = total_loss / len(train_loader)
        train_accuracy = correct_preds / total_preds
        train_losses.append(avg_train_loss)
        train_accuracies.append(train_accuracy)
        print(f"Training Loss: {avg_train_loss:.4f} | Training Accuracy: {train_accuracy:.4f}")

        # Validation
        model.eval()
        avg_val_loss = evaluate(model, val_loader, mode="Validation")
        val_losses.append(avg_val_loss)

        # Calculate validation accuracy
        val_correct_preds = 0
        val_total_preds = 0
        for batch in val_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            with torch.no_grad():
                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
                logits = outputs.logits

            preds = torch.argmax(logits, dim=1)
            val_correct_preds += (preds == labels).sum().item()
            val_total_preds += labels.size(0)

        val_accuracy = val_correct_preds / val_total_preds
        val_accuracies.append(val_accuracy)
        print(f"Validation Loss: {avg_val_loss:.4f} | Validation Accuracy: {val_accuracy:.4f}")

        # Early stopping check
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            patience_counter = 0
            best_model_state = model.state_dict()  # Save best model weights
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print(f"\nEarly stopping at epoch {epoch + 1}")
                model.load_state_dict(best_model_state)  # Restore best model
                break

    # Restore and save best model
    if best_model_state:
        model.load_state_dict(best_model_state)

        save_path = "best_model"
        model.save_pretrained(save_path)         # Save model
        tokenizer.save_pretrained(save_path)     # Save tokenizer
        print(f"\n Best model saved at: {save_path}")

    # Plot loss and accuracy
    plt.figure(figsize=(10, 6))

    # Loss plot
    plt.subplot(2, 1, 1)
    plt.plot(train_losses, label="Training Loss", marker='o')
    plt.plot(val_losses, label="Validation Loss", marker='o')
    plt.title("Training vs Validation Loss")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.legend()
    plt.grid(True)

    # Accuracy plot
    plt.subplot(2, 1, 2)
    plt.plot(train_accuracies, label="Training Accuracy", marker='o')
    plt.plot(val_accuracies, label="Validation Accuracy", marker='o')
    plt.title("Training vs Validation Accuracy")
    plt.xlabel("Epoch")
    plt.ylabel("Accuracy")
    plt.legend()
    plt.grid(True)

    plt.tight_layout()
    plt.show()

from sklearn.metrics import accuracy_score, classification_report
import torch

def evaluate(model, data_loader, mode="Test"):
    model.eval()  # Set the model to evaluation mode
    all_preds = []
    all_labels = []
    total_loss = 0

    with torch.no_grad():  # No gradient calculation during evaluation
        for batch in tqdm(data_loader, desc=mode):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            # Forward pass
            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)

            if outputs.loss is None:
                print(f"Warning: Loss is None at {mode}!")
                continue  # Skip this batch if loss is None

            loss = outputs.loss
            total_loss += loss.item()  # Accumulate loss

            logits = outputs.logits  # Model predictions
            preds = torch.argmax(logits, dim=1)  # Get the predicted labels (class with max logit)

            # Collect predictions and true labels
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    # Calculate average loss for the entire dataset
    avg_loss = total_loss / len(data_loader)

    # Calculate accuracy
    acc = accuracy_score(all_labels, all_preds)

    # Print metrics
    print(f"{mode} Loss: {avg_loss:.4f}")
    print(f"{mode} Accuracy: {acc:.4f}")
    print(f"{mode} Classification Report:\n{classification_report(all_labels, all_preds)}")

    return avg_loss  # Return the average loss for tracking

# Start training
best_val_loss = float('inf')  # Initialize best validation loss as infinity
num_epochs = 6  # number of epochs
train(model, train_dataloader, val_dataloader, optimizer, scheduler, num_epochs, tokenizer)

evaluate(model,test_dataloader, mode="Test")

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import torch

# Initialize lists to store true labels and predicted labels
y_true, y_pred = [], []

# Set the model to evaluation mode
model.eval()

# Disable gradient computation for evaluation
with torch.no_grad():
    # Loop through the test dataset
    for batch in test_dataloader:
        # Move data to the device (GPU/CPU)
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        # Get model outputs
        outputs = model(input_ids, attention_mask=attention_mask)

        # Get predictions (class with the highest logit value)
        preds = torch.argmax(outputs.logits, dim=1)

        # Extend lists with the true labels and predicted labels
        y_true.extend(labels.cpu().numpy())
        y_pred.extend(preds.cpu().numpy())

# Generate confusion matrix
cm = confusion_matrix(y_true, y_pred)

# Display confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["Fake", "Real"])
disp.plot(cmap=plt.cm.Blues)

# Set the title for the plot
plt.title("Confusion Matrix")

# Show the plot
plt.show()

model.save_pretrained("/content/drive/MyDrive/arabert-fake-news")
tokenizer.save_pretrained("/content/drive/MyDrive/arabert-fake-news")

!zip -r /content/best_model.zip /content/best_model2552025

from google.colab import files
files.download('/content/best_model.zip')

def predict(text, model, tokenizer):
    model.eval()
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512).to(device)
    with torch.no_grad():
        logits = model(**inputs).logits
        probs = torch.softmax(logits, dim=1)
        pred = torch.argmax(probs, dim=1).item()
    label = "Fake" if pred == 1 else "Real"
    confidence = probs[0][pred].item()
    print(f" Prediction: {label}\n Confidence: {confidence:.2f}")
    return label, confidence

model.eval()

#text = "Ø§Ù„Ø®Ø¨Ø± Ø§Ù„Ø°ÙŠ ØªØ±ÙŠØ¯ Ø§Ø®ØªØ¨Ø§Ø±Ù‡"
#predict(text, model, tokenizer)

sample_text = "Ù…ÙƒÙ† Ø§Ù„Ø¹Ù„Ù…Ø§Ø¡ Ù…Ù† Ø§ÙƒØªØ´Ø§Ù ØªÙ‚Ù†ÙŠØ© Ø­Ø¯ÙŠØ«Ø© ÙŠÙ…ÙƒÙ† Ø£Ù† ØªØ¹ÙŠØ¯ Ø§Ù„Ø´Ø¨Ø§Ø¨ Ø¥Ù„Ù‰ Ø³Ù† Ø§Ù„Ø¹Ø´Ø±ÙŠÙ†ØŒ Ù…Ù…Ø§ Ø³ÙŠØºÙŠØ± Ø­ÙŠØ§Ø© Ø§Ù„Ø¨Ø´Ø±ÙŠØ© Ø¨Ø§Ù„ÙƒØ§Ù…Ù„ ÙˆÙŠØ·ÙŠÙ„ Ù…Ù† Ø£Ø¹Ù…Ø§Ø± Ø§Ù„Ù†Ø§Ø³. Ø§Ù„ØªÙ‚Ù†ÙŠØ© ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ø¹Ø§Ø¯Ø© Ø¨Ø±Ù…Ø¬Ø© Ø§Ù„Ø®Ù„Ø§ÙŠØ§ ÙÙŠ Ø§Ù„Ø¬Ø³Ù…."
predict(sample_text, model, tokenizer)

text1= "ØªØ´ÙŠØ± Ø§Ù„Ø¯Ø±Ø§Ø³Ø§Øª Ø¥Ù„Ù‰ Ø£Ù† Ù‡Ù†Ø§Ùƒ Ø²ÙŠØ§Ø¯Ø© Ù…Ù„Ø­ÙˆØ¸Ø© ÙÙŠ Ø§Ù„ÙˆØ¹ÙŠ Ø§Ù„ØµØ­ÙŠ Ù„Ø¯Ù‰ Ø§Ù„Ø´Ø¨Ø§Ø¨ ÙÙŠ Ø§Ù„Ø¹Ø§Ù„Ù… Ø§Ù„Ø¹Ø±Ø¨ÙŠØŒ Ø­ÙŠØ« Ø£ØµØ¨Ø­ Ø§Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù†Ù‡Ù… ÙŠØ¹ØªÙ…Ø¯ÙˆÙ† Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù†Ø¸Ù…Ø© Ø§Ù„ØºØ°Ø§Ø¦ÙŠØ© Ø§Ù„Ù…ØªÙˆØ§Ø²Ù†Ø© ÙˆØ§Ù„ØªÙ…Ø§Ø±ÙŠÙ† Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ© Ø¨Ø´ÙƒÙ„ Ù…Ù†ØªØ¸Ù…."
predict(text1, model, tokenizer)

text2 = "Ø¯Ø±Ø§Ø³Ø© Ø¬Ø¯ÙŠØ¯Ø©: Ø´Ø±Ø¨ Ø§Ù„Ù…Ø§Ø¡ Ø§Ù„Ø³Ø§Ø®Ù† ÙŠÙ‚Ø¶ÙŠ Ø¹Ù„Ù‰ ÙÙŠØ±ÙˆØ³ ÙƒÙˆØ±ÙˆÙ†Ø§ ÙÙŠ Ø¯Ù‚Ø§Ø¦Ù‚"
predict(text2, model, tokenizer)

text3 = "Ø±Ø¦ÙŠØ³ Ø§Ù„ÙˆØ²Ø±Ø§Ø¡ ÙŠØ¤ÙƒØ¯ ÙÙŠ Ù…Ø¤ØªÙ…Ø± ØµØ­ÙÙŠ Ø¹Ù„Ù‰ Ø£Ù‡Ù…ÙŠØ© ØªØ¹Ø²ÙŠØ² Ø§Ù„Ø§Ù‚ØªØµØ§Ø¯ Ø§Ù„Ù…Ø­Ù„ÙŠ ÙˆØ¯Ø¹Ù… Ø§Ù„Ù…Ø´Ø§Ø±ÙŠØ¹ Ø§Ù„ØµØºÙŠØ±Ø©."
predict(text3, model, tokenizer)

text4 = "Ø´Ø±ÙƒØ© Ø¢Ø¨Ù„ ØªØ¹Ù„Ù† Ø¹Ù† Ø¥Ø·Ù„Ø§Ù‚ Ù‡Ø§ØªÙÙ‡Ø§ Ø§Ù„Ø¬Ø¯ÙŠØ¯ Ø¨ØªÙ‚Ù†ÙŠØ© ØªØ³Ù…Ø­ Ø¨Ø§Ù„Ø´Ø­Ù† Ù…Ù† Ø®Ù„Ø§Ù„ Ø§Ù„Ù‡ÙˆØ§Ø¡."
predict(text4, model, tokenizer)

text5 = "Ø§Ù„Ø³Ù„Ø·Ø§Øª Ø§Ù„Ø£Ø±Ø¯Ù†ÙŠØ© ØªÙ†ÙÙŠ Ø¥Ø´Ø§Ø¹Ø§Øª Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„Ø­Ø¯ÙˆØ¯ Ù…Ø¹ Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ© Ø¨Ø³Ø¨Ø¨ Ø§Ù„Ø£Ø­Ø¯Ø§Ø« Ø§Ù„Ø£Ø®ÙŠØ±Ø©."
predict(text5, model, tokenizer)

